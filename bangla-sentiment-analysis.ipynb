{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":95426,"databundleVersionId":11344607,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:35:34.344321Z","iopub.execute_input":"2025-04-20T10:35:34.345022Z","iopub.status.idle":"2025-04-20T10:35:34.351694Z","shell.execute_reply.started":"2025-04-20T10:35:34.344997Z","shell.execute_reply":"2025-04-20T10:35:34.351065Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/aiquest-bangla-sentiment-analysis-competition/sample_submission.csv\n/kaggle/input/aiquest-bangla-sentiment-analysis-competition/train.csv\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"train_df=pd.read_csv(\"/kaggle/input/aiquest-bangla-sentiment-analysis-competition/train.csv\")\ntest_df=pd.read_csv(\"/kaggle/input/aiquest-bangla-sentiment-analysis-competition/sample_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:35:34.352890Z","iopub.execute_input":"2025-04-20T10:35:34.353127Z","iopub.status.idle":"2025-04-20T10:35:34.367816Z","shell.execute_reply.started":"2025-04-20T10:35:34.353112Z","shell.execute_reply":"2025-04-20T10:35:34.367213Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"train_df\ntest_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:35:34.369039Z","iopub.execute_input":"2025-04-20T10:35:34.369285Z","iopub.status.idle":"2025-04-20T10:35:34.379346Z","shell.execute_reply.started":"2025-04-20T10:35:34.369256Z","shell.execute_reply":"2025-04-20T10:35:34.378665Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"      id sentiment\n0      0   neutral\n1      1   neutral\n2      2   neutral\n3      3   neutral\n4      4   neutral\n..   ...       ...\n184  184   neutral\n185  185   neutral\n186  186   neutral\n187  187   neutral\n188  188   neutral\n\n[189 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>184</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>185</th>\n      <td>185</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>186</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>187</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>188</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n<p>189 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"for i in range(1, 3):\n   globals()[f\"train{i}\"] = train_csv.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:35:34.380143Z","iopub.execute_input":"2025-04-20T10:35:34.380405Z","iopub.status.idle":"2025-04-20T10:35:34.395113Z","shell.execute_reply.started":"2025-04-20T10:35:34.380384Z","shell.execute_reply":"2025-04-20T10:35:34.394412Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"import os\nimport re\nimport gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport unicodedata\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\nimport nltk\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:35:34.396570Z","iopub.execute_input":"2025-04-20T10:35:34.396758Z","iopub.status.idle":"2025-04-20T10:35:34.408180Z","shell.execute_reply.started":"2025-04-20T10:35:34.396744Z","shell.execute_reply":"2025-04-20T10:35:34.407536Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Download necessary NLTK data\nnltk.download('punkt')\nnltk.download('stopwords')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:35:34.408817Z","iopub.execute_input":"2025-04-20T10:35:34.409087Z","iopub.status.idle":"2025-04-20T10:35:34.424422Z","shell.execute_reply.started":"2025-04-20T10:35:34.409064Z","shell.execute_reply":"2025-04-20T10:35:34.423751Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"# Bangla text cleaner\ndef bangla_cleaner(text):\n    stop_words = set(stopwords.words('bengali'))\n    text = re.sub(r'\\|?See Translation', '', text)\n    text = text.lower()\n    text = re.sub(r'[a-z0-9_]+', '', text)\n    text = re.sub(r'[।॥!?.,;:\\\"\\'‘’—…()\\[\\]{}<>@#$%^&*_+=|\\\\/~`]', '', text)\n    text = re.sub(r'[^ঀ-৿\\s]', '', text)\n    text = unicodedata.normalize('NFKC', text)\n    tokens = word_tokenize(text)\n    tokens = [word for word in tokens if word not in stop_words]\n    return ' '.join(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:35:34.425082Z","iopub.execute_input":"2025-04-20T10:35:34.425305Z","iopub.status.idle":"2025-04-20T10:35:34.440173Z","shell.execute_reply.started":"2025-04-20T10:35:34.425290Z","shell.execute_reply":"2025-04-20T10:35:34.439438Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# Clean text\ntrain_df['clean_text'] = train_df['text'].apply(bangla_cleaner)\ntest_df['clean_text'] = test_df['sentiment'].apply(bangla_cleaner)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:35:34.440847Z","iopub.execute_input":"2025-04-20T10:35:34.441551Z","iopub.status.idle":"2025-04-20T10:35:34.544873Z","shell.execute_reply.started":"2025-04-20T10:35:34.441534Z","shell.execute_reply":"2025-04-20T10:35:34.544196Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Label encoding\nle = LabelEncoder()\ntrain_df['label'] = le.fit_transform(train_df['sentiment'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:35:34.545613Z","iopub.execute_input":"2025-04-20T10:35:34.545844Z","iopub.status.idle":"2025-04-20T10:35:34.550192Z","shell.execute_reply.started":"2025-04-20T10:35:34.545823Z","shell.execute_reply":"2025-04-20T10:35:34.549404Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Tokenizer\nmodel_name = \"csebuetnlp/banglabert\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:35:34.551691Z","iopub.execute_input":"2025-04-20T10:35:34.551978Z","iopub.status.idle":"2025-04-20T10:35:34.726870Z","shell.execute_reply.started":"2025-04-20T10:35:34.551957Z","shell.execute_reply":"2025-04-20T10:35:34.726341Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# Dataset class\nclass BanglaDataset(Dataset):\n    def __init__(self, texts, labels=None):\n        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=128)\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels is not None:\n            item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:35:34.727448Z","iopub.execute_input":"2025-04-20T10:35:34.727621Z","iopub.status.idle":"2025-04-20T10:35:34.732380Z","shell.execute_reply.started":"2025-04-20T10:35:34.727608Z","shell.execute_reply":"2025-04-20T10:35:34.731777Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# Cross-validation setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ntest_preds = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train_df['clean_text'], train_df['label'])):\n    print(f\"\\nFold {fold+1}\")\n    train_texts = train_df.iloc[train_idx]['clean_text'].tolist()\n    val_texts = train_df.iloc[val_idx]['clean_text'].tolist()\n    train_labels = train_df.iloc[train_idx]['label'].tolist()\n    val_labels = train_df.iloc[val_idx]['label'].tolist()\n\n    train_dataset = BanglaDataset(train_texts, train_labels)\n    val_dataset = BanglaDataset(val_texts, val_labels)\n\n    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:35:34.733333Z","iopub.execute_input":"2025-04-20T10:35:34.733587Z","iopub.status.idle":"2025-04-20T10:35:39.510194Z","shell.execute_reply.started":"2025-04-20T10:35:34.733565Z","shell.execute_reply":"2025-04-20T10:35:39.509387Z"}},"outputs":[{"name":"stdout","text":"\nFold 1\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nFold 2\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nFold 3\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nFold 4\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nFold 5\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"!pip install transformers --upgrade\ntraining_args = TrainingArguments(\n    output_dir=f\"./results_fold_{fold+1}\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    logging_dir=f\"./logs_fold_{fold+1}\",\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    # The 'evaluation_strategy' argument was introduced in a later version\n    # of Transformers. For older versions, you might need to specify it\n    # differently or omit it altogether.\n    # evaluation_strategy=\"epoch\", \n    report_to=\"none\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:35:39.511103Z","iopub.execute_input":"2025-04-20T10:35:39.511347Z","iopub.status.idle":"2025-04-20T10:35:42.779338Z","shell.execute_reply.started":"2025-04-20T10:35:39.511331Z","shell.execute_reply":"2025-04-20T10:35:42.778458Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        preds = np.argmax(logits, axis=1)\n        return {\n            'accuracy': accuracy_score(labels, preds),\n            'f1': f1_score(labels, preds, average='macro')\n        }\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n    trainer.evaluate()\n\n    # Predict on test\n    test_dataset = BanglaDataset(test_df['clean_text'].tolist())\n    preds = trainer.predict(test_dataset).predictions\n    test_preds.append(torch.softmax(torch.tensor(preds), dim=1).numpy())\n\n    del model\n    torch.cuda.empty_cache()\n    gc.collect()\n\n# Average predictions\nfinal_preds = np.mean(test_preds, axis=0)\npred_labels = np.argmax(final_preds, axis=1)\ndecoded_preds = le.inverse_transform(pred_labels)\n\n# Create submission\nsubmission = pd.DataFrame({\n    \"id\": test_df[\"id\"],\n    \"sentiment\": decoded_preds\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"submission.csv created!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:35:42.781446Z","iopub.execute_input":"2025-04-20T10:35:42.781700Z","iopub.status.idle":"2025-04-20T10:36:02.559207Z","shell.execute_reply.started":"2025-04-20T10:35:42.781678Z","shell.execute_reply":"2025-04-20T10:36:02.558448Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15/15 00:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.016700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"submission.csv created!\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}